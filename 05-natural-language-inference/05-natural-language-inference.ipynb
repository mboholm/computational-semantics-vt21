{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Inference using Neural Networks\n",
    "Adam Ek\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Before starting, please read the instructions on [how to work on group assignments](https://github.com/sdobnik/computational-semantics/blob/master/README.md).\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we'll work with neural networks for natural language inference. Our task is: given a premise sentence P and hypothesis H, what entailment relationship holds between them? Is H entailed by P, contradicted by P or neutral towards P?\n",
    "\n",
    "Given a sentence P, if H definitely describe something true given P then it is an **entailment**. If H describe something that's *maybe* true given P, it's **neutral**, and if H describe something that's definitely *false* given P it's a **contradiction**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: **MAX BOHOLM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore natural language inference using neural networks on the SNLI dataset, described in [1]. The dataset can be downloaded [here](https://nlp.stanford.edu/projects/snli/). We prepared a \"simplified\" version, with only the relevant columns [here](https://gubox.box.com/s/idd9b9cfbks4dnhznps0gjgbnrzsvfs4).\n",
    "\n",
    "The (simplified) data is organized as follows (tab-separated values):\n",
    "* Column 1: Premise\n",
    "* Column 2: Hypothesis\n",
    "* Column 3: Relation\n",
    "\n",
    "Like in the previous lab, we'll use torchtext to build a dataloader. You can essentially do the same thing as you did in the last lab, but with our new dataset. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 3\n",
    "my_dimensions = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** From time to time (or rather most of the time), there is a memory problem on MLTGPU, resulting in the following error:\n",
    "\n",
    "    RuntimeError: CUDA error: out of memory\n",
    "\n",
    "I have had runs of the code all the way through with (if I remember correctly): the complete trainingset, `my_dimensions = 100`, `batch_size = 16`, and `epochs = 3`, resulting in an accuracy of about 48%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can ignore this function**, but `mini_me()` is useful when developing the model with a smaller data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    import random\n",
    "\n",
    "    def mini_me(cutoff, directory=\"snli-data\", original=\"train.csv\", out=\"mini_me.csv\"):\n",
    "        with open(f\"{directory}/{original}\", mode=\"r\") as f:\n",
    "            data=[x.split(\"\\t\") for x in f.read().split(\"\\n\")]\n",
    "\n",
    "        random.shuffle(data)\n",
    "\n",
    "        with open(f\"{directory}/{out}\", mode=\"w\") as f:\n",
    "            f.write(\"\\n\".join([\"\\t\".join(x) for x in data][:cutoff]))\n",
    "\n",
    "    mini_me(cutoff=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In order to avoid \n",
    "\n",
    "    AttributeError: 'Example' object has no attribute 'context'\n",
    "\n",
    "when iterating over the train and test data from the `dataloader` I have removed empty lines in the `train.csv` and `test.csv` files. I could have implemented some code here to do that from the Jupyter Notebook file, but I have not doe that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchtext.legacy.data import Field, BucketIterator, Iterator, TabularDataset # Needed for running this on my laptop\n",
    "from torchtext.data import Field, BucketIterator, Iterator, TabularDataset\n",
    "\n",
    "def dataloader(directory=\"snli-data\",\n",
    "               train_file=\"train.csv\",\n",
    "               #train_file=\"mini_me.csv\",\n",
    "               test_file=\"test.csv\",\n",
    "               batch=batch_size):\n",
    "    \n",
    "    whitespacer = lambda x: x.split(' ') #from: https://canvas.gu.se/files/4597768/download?download_frd=1\n",
    "  \n",
    "    SENTENCE = Field(tokenize   = whitespacer,\n",
    "                    lower       = True,\n",
    "                    batch_first = True,\n",
    "                    init_token  = \"<start>\", \n",
    "                    eos_token   = \"<end>\"\n",
    "                   ) \n",
    "    \n",
    "    LABEL = Field(batch_first = True)    \n",
    "    \n",
    "    my_fields = [(\"premise\", SENTENCE),\n",
    "                 (\"hypothesis\", SENTENCE),\n",
    "                 (\"label\", LABEL)]\n",
    "    \n",
    "    train, test = TabularDataset.splits(path   = directory,\n",
    "                                        train  = train_file,\n",
    "                                        test   = test_file,\n",
    "                                        format = 'csv',\n",
    "                                        fields = my_fields,\n",
    "                                        csv_reader_params = {'delimiter':'\\t',\n",
    "                                                             'quotechar':'¤'}) \n",
    "                                        #\"¤\" not in data\n",
    "    SENTENCE.build_vocab(train) \n",
    "    LABEL.build_vocab(train)\n",
    "\n",
    "    train_iter, test_iter = BucketIterator.splits((train, test),\n",
    "                                                  batch_size        = batch,\n",
    "                                                  sort_within_batch = True,\n",
    "                                                  sort_key          = lambda x: len(x.premise),\n",
    "                                                  shuffle           = True,\n",
    "                                                  device            = device)\n",
    "\n",
    "    return train_iter, test_iter, SENTENCE.vocab, LABEL.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll build the model for predicting the relationship between H and P.\n",
    "\n",
    "We will process each sentence using an LSTM. Then, we will construct some representation of the sentence. When we have a representation for H and P, we will combine them into one vector which we can use to predict the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a model described in [2], the BiLSTM with max-pooling model. The procedure for the model is roughly:\n",
    "\n",
    "    1) Encode the Hypothesis and the Premise using one shared bidirectional LSTM (or two different LSTMS)\n",
    "    2) Perform max over the tokens in the premise and the hypothesis\n",
    "    3) Combine the encoded premise and encoded hypothesis into one representation\n",
    "    4) Predict the relationship "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a representation of a sentence\n",
    "\n",
    "Let's first consider step 2 where we perform max/mean pooling. There is a function in pytorch for this, but we'll implement it from scratch. \n",
    "\n",
    "Let's consider the general case, what we want to do for these methods is apply some function $f$ along dimension $i$, and we want to do this for all $i$'s. As an example we consider the matrix S with size ``(N, D)`` where N is the number of words and D the number of dimensions:\n",
    "\n",
    "$S = \\begin{bmatrix}\n",
    "    s_{11} & s_{12} & s_{13} & \\dots  & s_{1d} \\\\\n",
    "    s_{21} & s_{22} & s_{23} & \\dots  & s_{2d} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    s_{n1} & s_{n2} & s_{n3} & \\dots  & s_{nd}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "What we want to do is apply our function $f$ on each dimension, taking the input $s_{1d}, s_{2d}, ..., s_{nd}$ and generating the output $x_d$. \n",
    "\n",
    "You will implement both the max pooling method. When performing max-pooling, $max$ will be the function which selects a _maximum_ value from a vector and $x$ is the output, thus for each dimension $d$ in our output $x$ we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    x_d = max(s_{1d}, s_{2d}, ..., s_{nd})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "This operation will reduce a batch of size ``(batch_size, num_words, dimensions)`` to ``(batch_size, dimensions)`` meaning that we now have created a sentence representation based on the content of the words representations in the sentence. \n",
    "\n",
    "Create a function that takes as input a tensor of size ``(batch_size, num_words, dimensions)`` then performs max pooling and returns the result (the output should be of size: ```(batch_size, dimensions)```). [**4 Marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(input_tensor):\n",
    "    output_tensor=torch.max(input_tensor, dim=1).values #`keepdim=False` by default, which gives us squeezed output\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining sentence representations\n",
    "\n",
    "Next, we need to combine the premise and hypothesis into one representation. We will do this by concatenating four tensors (the final size of our tensor $X$ should be ``(batch_size, 4d)`` where ``d`` is the number of dimensions that you use): \n",
    "\n",
    "$$X = [P; H; |P-H|; P \\cdot H]$$\n",
    "\n",
    "Here, what we do is concatenating P, H, P times H, and the absolute value of P minus H, then return the result.\n",
    "\n",
    "Implement the function. **[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_premise_and_hypothesis(premise, hypothesis):\n",
    "    \n",
    "    abs_difference=torch.abs(premise - hypothesis)\n",
    "    multiplied=premise*hypothesis\n",
    "    \n",
    "    output=torch.cat((premise, hypothesis, abs_difference, multiplied), dim=1)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "Finally, we can build the model according to the procedure given previously by using the functions we defined above. Additionaly, in the model you should use *dropout*. For efficiency purposes, it's acceptable to only train the model with either max or mean pooling. \n",
    "\n",
    "Implement the model [**6 marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIModel(nn.Module):\n",
    "    def __init__(self, voc_size, n_dimensions, n_labels):\n",
    "        super(SNLIModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(voc_size, n_dimensions)\n",
    "        self.rnn = nn.LSTM(n_dimensions, n_dimensions, bidirectional=True, batch_first=True)\n",
    "        self.classifier = nn.Linear(n_dimensions*2*4, n_labels)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, premise, hypothesis):\n",
    "        p = self.embeddings(premise)\n",
    "        h = self.embeddings(hypothesis)\n",
    "        \n",
    "        seq_p, *_ = self.rnn(p)\n",
    "        seq_h, *_ = self.rnn(h)\n",
    "        \n",
    "        p_pooled = pooling(seq_p)\n",
    "        h_pooled = pooling(seq_h)\n",
    "        \n",
    "        ph_representation = combine_premise_and_hypothesis(p_pooled, h_pooled)\n",
    "        \n",
    "        drop = self.dropout(ph_representation)\n",
    "        \n",
    "        predictions = self.classifier(drop)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, implement the training and testing of the model. SNLI can take a very long time to train, so I suggest you only run it for one or two epochs. **[2 marks]** \n",
    "\n",
    "**Tip for efficiency:** *when developing your model, try training and testing the model on one batch (for each epoch) of data to make sure everything works! It's very annoying if you train for N epochs to find out that something went wrong when testing the model, or to find that something goes wrong when moving from epoch 0 to epoch 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.848468930197434\n",
      "51.987904647312946\n",
      "92.962872322101166\n"
     ]
    }
   ],
   "source": [
    "#Training...\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "train_iter, test_iter, vocab, labels = dataloader()\n",
    "\n",
    "model = SNLIModel(voc_size=len(vocab),\n",
    "                  n_dimensions=my_dimensions,\n",
    "                  n_labels=len(labels))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_loss = 0\n",
    "for e in range(epochs):\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        #print(len(batch))\n",
    "        p = batch.premise\n",
    "        h = batch.hypothesis\n",
    "        label = batch.label\n",
    "        \n",
    "        output = model(p, h)\n",
    "        #print(output)\n",
    "        #print(label)\n",
    "        \n",
    "        loss = loss_function(output, label.squeeze())\n",
    "        \n",
    "        # For a batch sized 1, there needs to do some transformations \n",
    "        # so that the loss function does not complain\n",
    "        # Since, the solution below is suboptimal, it is\n",
    "        # commented out.\n",
    "        # if len(batch) == 1:  \n",
    "        #     loss = loss_function(output, label.squeeze(1))\n",
    "        # else:\n",
    "        #     loss = loss_function(output, label.squeeze()) \n",
    "        \n",
    "        #Note: code below adopted from previous assignment\n",
    "        total_loss += loss.item()\n",
    "        print(total_loss/(i+1), end='\\r') \n",
    "        loss.backward() # compute gradients\n",
    "        optimizer.step() # update parameters\n",
    "        optimizer.zero_grad # reset gradients\n",
    "        \n",
    "        #break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3986\n",
      "\n",
      "Relation\tAccuracy\n",
      "contradiction\t0.1742354031510658\n",
      "neutral\t0.1186703945324635\n",
      "entailment\t0.9026128266033254\n"
     ]
    }
   ],
   "source": [
    "#Testing ...\n",
    "correct_set = []\n",
    "correct_per_relation = {label:[] for label in [labels.itos[x] for x in range(len(labels))]}\n",
    "model.eval() #evaluation mode\n",
    "\n",
    "for i, batch in enumerate(test_iter):\n",
    "    print(f\"{round((i/len(test_iter))*100, 3)} %\", end=\"\\r\")\n",
    "    p = batch.premise\n",
    "    h = batch.hypothesis\n",
    "    label = batch.label\n",
    "    \n",
    "    output = model(p, h)\n",
    "    \n",
    "    my_probs = F.softmax(output, dim=1)\n",
    "    index_of_top_prob = torch.max(my_probs, dim=1).indices\n",
    "    predicted_label = [labels.itos[x] for x in index_of_top_prob]\n",
    "    if len(batch) == 1:\n",
    "        true_label = [labels.itos[x] for x in label.squeeze(1)]\n",
    "    else:\n",
    "        true_label = [labels.itos[x] for x in label.squeeze()]\n",
    "    \n",
    "    for prediction, truth in zip(predicted_label, true_label):\n",
    "        if prediction == truth:\n",
    "            correct_set.append(1)\n",
    "            correct_per_relation[truth].append(1)\n",
    "        else:\n",
    "            correct_set.append(0)\n",
    "            correct_per_relation[truth].append(0)\n",
    "\n",
    "accuracy = sum(correct_set) / len(correct_set)\n",
    "\n",
    "accuracy_per_relation = {label:0 for label in correct_per_relation.keys()}\n",
    "\n",
    "for label in correct_per_relation.keys():\n",
    "    if len(correct_per_relation[label]) == 0:\n",
    "        accuracy_per_relation[label] = \"NA\"\n",
    "    else:\n",
    "        mean = sum(correct_per_relation[label]) / len(correct_per_relation[label])\n",
    "        accuracy_per_relation[label] = mean\n",
    "    \n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print()\n",
    "print(\"Relation\\tAccuracy\")\n",
    "\n",
    "for relation in [\"contradiction\", \"neutral\", \"entailment\"]:\n",
    "    print(\"{}\\t{}\".format(relation, accuracy_per_relation[relation])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest a _baseline_ that we can compare our model against **[2 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**\n",
    "\n",
    "*Naive Model* (Baseline): for every instance predict the most common relation (label) of the data (*Lmax*). \n",
    "\n",
    "Naive Model would get an accuacy of *count*(*Lmax*) / N. With three labels in balanced test and training samples where the labels are equally common, as the present ones, accuracy of Naive Model would be about 1/3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333914990766188\n"
     ]
    }
   ],
   "source": [
    "#A baseline\n",
    "with open(\"snli-data/train.csv\", mode=\"r\") as f:\n",
    "    data = [x.split(\"\\t\") for x in f.read().split(\"\\n\")]\n",
    "\n",
    "counter = {}\n",
    "for x in data:\n",
    "    relation = x[-1]\n",
    "    if relation in counter:\n",
    "        counter[relation]+=1\n",
    "    else:\n",
    "        counter[relation]=1\n",
    "\n",
    "#print(counter)\n",
    "baseline = max(counter.values()) / len(data)\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways (other than using a baseline) in which we can analyse the models performance **[4 marks]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**\n",
    "\n",
    "A number of variations for evaluation of NLI models other than the simple train-and-test procedure implemented here have been discussed in the litterature (Talman et al, 2019 has been the main inspiration for this list).\n",
    "\n",
    "1. The model can be trained and tested on various datasets othe than SNLI, e.g. *MultiNLI* and *SciTail*.\n",
    "2. The model could be trained on one dataset, but evaluated on another. Annotation artifacts is a known problem of present datasets. Cross-dataset evaluation is a procedure to address this problem.\n",
    "3. Detailed error analysis of what inference labels are handled best/worst in various datasets.\n",
    "4. Linguisitc features analysis, including breaking tests or \"stress tests\" (Naik et al. 2018), using datasets (e.g. Breaking NLI) which has been designed to incorporate linguitic features known to be challenging for NLI, such as word overlap, negation, and antonymy. \n",
    "5. Transfer experiments. Given that the NLI task is designed so that sentence representations are treated separately, the sentence embeddings can be used in a in transfer learning in a downstream task. Thus, our model can be evaluated based on the preformance of the sentence representations it produces in tasks over and above NLI, e.g. in SentEval dataset. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways to improve the model **[3 marks]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here**\n",
    "\n",
    "1. Conneau et al. (also Talman et al.) achieves high performances by implementing hierarchical structure of with several layers of LSTMs.\n",
    "2. Talman et al. implements an iterative refinement architecture, where the input is \"reconsidered\" at deeper layers of LSTMs. They found this hierarchical structure to out outperform other hirearchical layouts (incl. stacked LSTMs layers).\n",
    "3. Using pre-trained word embeddings is a possible venue for improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional references ...\n",
    "\n",
    "Naik A. et al 2018. \"Stress test evaluation for natural language inference\", *Proceedings of the 27th International Conference on Computational Linguistics*, pp. 2340-2353. \n",
    "\n",
    "Talman A. et al 2019. \"Sentence embeddings in NLI with iterative refinement encoders\", *Natural Language Engineering*, 25: 467-482"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readings\n",
    "\n",
    "[1] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). \n",
    "\n",
    "[2] Conneau, A., Kiela, D., Schwenk, H., Barrault, L., & Bordes, A. (2017). Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
