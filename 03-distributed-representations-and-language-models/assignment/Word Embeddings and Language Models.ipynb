{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Word Embeddings and Language Modelling\n",
    "\n",
    "Adam Ek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we'll explore constructing *static* word embeddings (i.e. word2vec) and building language models. We'll also evaluate these systems on intermediate tasks, namely word similarity and identifying \"good\" and \"bad\" sentences.\n",
    "\n",
    "* For this we'll use pytorch. Some basic operations that will be useful can be found here: https://jhui.github.io/2018/02/09/PyTorch-Basic-operations\n",
    "* In general: we are not interested in getting state-of-the-art performance :) focus on the implementation and not results of your model. For this reason, you can use a subset of the dataset: the first 5000-10 000 sentences or so, on linux/mac: ```head -n 10000 inputfile > outputfile```. \n",
    "* If possible, use the MLTGpu, it will make everything faster :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# for gpu, replace \"cpu\" with \"cuda:n\" where n is the index of the GPU\n",
    "#device = torch.device('cpu')\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE:\n",
    "There has been space issues on MLTGPU while working with this assignment. For example, \n",
    "\n",
    "    RuntimeError: CUDA out of memory\n",
    "\n",
    "Therefore it has been hard to train models with large datasets and batch sizes. The cell below makes it possible to a subset of sentences and a batch size at one place.\n",
    "\n",
    "Presently (May 14, evening) the sitaution seems to have appoved. I have been able to train the CBOW model with the complete 50k dataset and batch size 16. When coming to the Language Model, CUDA out of memory, so I changed batch size to 3, from first 16, and second 8. And then data to 10k lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model10k16b\n"
     ]
    }
   ],
   "source": [
    "my_restriction = False # Set to False for no restriction of data; set to True for using n_subsamle as restriction\n",
    "n_subsample = 10000 #Sub-sample\n",
    "n_batch = 16 #Batch Size\n",
    "name_of_model = f\"model{str(n_subsample)[:-3]}k{n_batch}b\"\n",
    "print(name_of_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec embeddings\n",
    "\n",
    "In this first part we'll construct a word2vec model which will give us *static* word embeddings (that is, they are fixed after training).\n",
    "\n",
    "After we've trained our model we will evaluate the embeddings obtained on a word similarity task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load some data, you can download the file on canvas under files/03-lab-data/wiki-corpus.txt. The file contains 50 000 sentences randomly selected from the complete wikipedia. Each line in the file contains one sentence. The sentences are whitespace tokenized.\n",
    "\n",
    "Your first task is to create a dataset suitable for word2vec. That is, we define some ```window_size``` then iterate over all sentences in the dataset, putting the center word in one field and the context words in another (separate the fields with ```tab```).\n",
    "\n",
    "For example, the sentece \"this is a lab\" with ```window size = 4``` will be formatted as:\n",
    "```\n",
    "center, context\n",
    "---------------------\n",
    "this    is a\n",
    "is      this a lab\n",
    "a       this is lab\n",
    "lab     is a\n",
    "```\n",
    "\n",
    "this will be our training examples when training the word2vec model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/wiki-corpus.txt'\n",
    "WINDOW_SIZE = 4 #on each side!\n",
    "def corpus_reader(data_path, output=\"data/my_data.csv\", k=WINDOW_SIZE, restriction=my_restriction):\n",
    "    \n",
    "    with open(data_path, mode=\"r\") as f:\n",
    "        my_data=[line.split(\" \") for line in f.read().split(\"\\n\")]\n",
    "    \n",
    "    if restriction == True:\n",
    "        my_data=my_data[:n_subsample] #OBS!\n",
    "    \n",
    "    my_string=\"\"\n",
    "    for sentence in my_data:\n",
    "        if len(sentence)>2: \n",
    "            # Only at least two-word contexts (minimal context). \n",
    "            # The simple reason for this is to solve downstream problems otherwise\n",
    "            # encounter with training related to the CrossEntropyLoss function \n",
    "            my_context=[\"<x>\"]*k + sentence + [\"<x>\"]*k\n",
    "            i=k\n",
    "            for w in sentence:\n",
    "                my_string+=w+\"\\t\"\n",
    "                j=i+1\n",
    "                left=my_context[i-k:i]\n",
    "                right=my_context[j:j+k]\n",
    "                context_words=[c for c in left+right if c not in[\"<x>\", \"(\", \")\", \".\", \"!\", \",\", '\"', \"'\", \"?\"]]\n",
    "                my_string+=\" \".join(context_words)+\"\\n\"\n",
    "                i+=1\n",
    "    \n",
    "    with open(output, mode=\"w\") as f:\n",
    "        f.write(my_string[:-1]) #note to self: we do not want empty rows\n",
    "\n",
    "corpus_reader(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sampled 50 000 senteces completely random from the *whole* wikipedia for our training data. Give some reasons why this is good, and why it might be bad. (*note*: We'll have a few questions like these, one or two reasons for and against is sufficient)\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "Here are some ideas for why it might not always be a good idea:\n",
    "1.    Wikipedia illutrates a restricted register of language use and its representativity can be discussed (as always in corpus linguistics).\n",
    "2.    The diversified collection might perhaps result in a high Type Token Ratio (TTR), i.e. number of types / number of tokens. For our present purposes, I suppose that a high TTR can mean that we have many word types, but a limited context to learn (generalize) their embeddings from.\n",
    "\n",
    "The main good reasons derive from availibility:\n",
    "\n",
    "*    Wikipedia has a lot of text\n",
    "*    Wikipedia pages and their URLs are stricly standardized, which makes Web crawling and text extraction easy\n",
    "\n",
    "Also, Wikipedia is commonly used as data in NLP tasks, which makes improves possibilities for comparisons "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "We now need to load the data in an appropriate format for torchtext (https://torchtext.readthedocs.io/en/latest/). We'll use PyText for this and it'll follow the same structure as I showed you in the lecture (remember to lower-case all tokens). Create a function which returns a (bucket)iterator of the training data, and the vocabulary object (```Field```). \n",
    "\n",
    "(*hint1*: you can format the data such that the center word always is first, then you only need to use one field)\n",
    "\n",
    "(*hint2*: the code I showed you during the leture is available in /files/pytorch_tutorial/ on canvas)\n",
    "\n",
    "[4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchtext.legacy.data import Field, BucketIterator, Iterator, TabularDataset # Needed for running this on my laptop\n",
    "from torchtext.data import Field, BucketIterator, Iterator, TabularDataset\n",
    "\n",
    "def get_data(\n",
    "    my_path = \"data/my_data.csv\",\n",
    "    batch_size = 3\n",
    "    ):\n",
    "    \n",
    "    whitespacer = lambda x: x.split(' ') #from: files/partofspeech-tagging_main.py on Canvas\n",
    "   \n",
    "    MY_FIELD = Field(\n",
    "        tokenize = whitespacer,\n",
    "        lower=True,\n",
    "        batch_first = True\n",
    "        )\n",
    "        \n",
    "    my_fields = [(\"target_word\", MY_FIELD), (\"context_words\", MY_FIELD)]   \n",
    "    \n",
    "    train = TabularDataset(\n",
    "        path   = my_path,\n",
    "        format = 'csv',\n",
    "        fields = my_fields,\n",
    "        #skip_header = True,\n",
    "        csv_reader_params = {'delimiter':'\\t', 'quotechar':'{'} #Note \"{\" is used as quotechar\n",
    "        )\n",
    "    \n",
    "    #Note on quote character selection: \n",
    "    #By trial and error I found that \"{\" is not a character in the corpus.\n",
    "    #Therefore it can be used as quote character. (Swedish \"å\", \"ä\" and \"ö\" are in \n",
    "    #the corpus. So is \"½\".)\n",
    "\n",
    "    MY_FIELD.build_vocab(train, min_freq=3)\n",
    "    \n",
    "    my_bucket = BucketIterator(\n",
    "        train,      \n",
    "        batch_size        = batch_size,\n",
    "        sort_within_batch = True,\n",
    "        sort_key          = lambda x: len(x.context_words),\n",
    "        shuffle           = True,\n",
    "        device            = device)    \n",
    "    \n",
    "    return my_bucket, MY_FIELD.vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lower-cased all tokens above; give some reasons why this is a good idea, and why it may be harmful to our embeddings.\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** \n",
    "\n",
    "Reasons *against* lower-casing: when lower-casing, at least two types of information that are *lost*:\n",
    "*    Named entities (e.g. *Shell* vs *shell*)\n",
    "*    Sentence bounary information (e.g. *The* is a better predicition after *.* than *the*).\n",
    "\n",
    "In as far as such information is important for the task we are trying to solve with the embeddings, lower-casing can be \"harmfull\". Consider, for example, named entitiy recognition and language modelling aiming to predict sequentiality of language.\n",
    "\n",
    "On the other hand, upper case does not in general carry much relevant information. Mostly, the upper vs. lower case is *semantically* redundant, but *expressively* expensive. Potentially our vocabulary could be twice as long. As such, there is good reason for lower-casing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the CBOW model for constructing word embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the CBOW model we try to predict the center word based on the context. That is, we take as input ```n``` context words, encode them as vectors, then combine them by summation. This will give us one embedding. We then use this embedding to predict *which* word in our vocabuary is the most likely center word. \n",
    "\n",
    "Implement this model \n",
    "\n",
    "[7 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, voc_size, hidden_d):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(voc_size, hidden_d) #vocabulary size * hidden size\n",
    "        self.prediction = nn.Linear(hidden_d, voc_size)\n",
    "        #self.vsize=voc_size # Please ignore. I used this for my first projection function.\n",
    "        \n",
    "        # NOTE: Softmax is part of the loss function implemented below\n",
    "    \n",
    "    def forward(self, context):\n",
    "        embedded_context = self.embeddings(context)\n",
    "        projection = self.projection_function2(embedded_context)\n",
    "        predictions = self.prediction(projection)\n",
    "       \n",
    "        return predictions\n",
    "    \n",
    "    def projection_function2(self, xs):\n",
    "        \"\"\"\n",
    "        This function will take as input a tensor of size (B, S, D)\n",
    "        where B is the batch_size, S the window size, and D the dimensionality of embeddings\n",
    "        this function should compute the sum over the embedding dimensions of the input, \n",
    "        that is, we transform (B, S, D) to (B, 1, D) or (B, D) \n",
    "        \"\"\"\n",
    "        #Note: Implemented as suggested in class.\n",
    "        \n",
    "        xs_sum = torch.sum(xs, dim=1) # helpful reference: https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be\n",
    "        \n",
    "        return xs_sum   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to train the models. First we define which hyperparameters to use. (You can change these, for example when *developing* your model you can use a batch size of 2 and a very low dimensionality (say 10), just to speed things up). When actually training your model *fo real*, you can use a batch size of [8,16,32,64], and embedding dimensionality of [128,256]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs :)\n",
    "word_embeddings_hyperparameters = {'epochs':3,\n",
    "                                   'batch_size':n_batch, \n",
    "                                   'embedding_size':128,\n",
    "                                   'learning_rate':0.001,\n",
    "                                   'embedding_dim':128}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model. Iterate over the dataset, get outputs from your model, calculate loss and backpropagate.\n",
    "\n",
    "We mentioned in the lecture that we use Negative Log Likelihood (https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) loss to train Word2Vec model. In this lab we'll take a shortcut when *training* and use Cross Entropy Loss (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), basically it combines ```log_softmax``` and ```NLLLoss```. So what your model should output is a *score* for each word in our vocabulary. The ```CrossEntropyLoss``` will then assign probabilities and calculate the negative log likelihood loss.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary:  80707\n",
      "13353.605640116202\n",
      "62035.211929631836\n",
      "140511.48417805787\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "dataset, vocab = get_data(batch_size = word_embeddings_hyperparameters['batch_size'])\n",
    "\n",
    "print(\"Length of vocabulary: \", len(vocab))\n",
    "\n",
    "# build model and construct loss/optimizer\n",
    "cbow_model = CBOWModel(len(vocab), word_embeddings_hyperparameters['embedding_dim'])\n",
    "cbow_model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=word_embeddings_hyperparameters['learning_rate'])\n",
    "\n",
    "# start training loop\n",
    "cbow_model.train()\n",
    "total_loss = 0\n",
    "for epoch in range(word_embeddings_hyperparameters['epochs']):\n",
    "    for i, batch in enumerate(dataset):\n",
    "        \n",
    "        context = batch.context_words\n",
    "        target_word = batch.target_word\n",
    "        \n",
    "        # send your batch of sentences to the model\n",
    "        output = cbow_model(context)\n",
    "        \n",
    "        # compute the loss, you'll need to reshape the input\n",
    "        # you can read more about this is the documentation for\n",
    "        # CrossEntropyLoss\n",
    "\n",
    "        loss = loss_fn(output, target_word.squeeze())\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # print average loss for the epoch\n",
    "        print(total_loss/(i+1), end='\\r') \n",
    "        \n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # reset gradients\n",
    "        optimizer.zero_grad\n",
    "        \n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING THE MODEL\n",
    "#PATH = f\"models/{name_of_model}.pt\"\n",
    "#torch.save(cbow_model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR LOADING THE MODEL\n",
    "#model = torch.load(PATH)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the model on a dataset of word similarities, WordSim353 (http://alfonseca.org/eng/research/wordsim353.html , also avalable in vanvas under files/03-l). The first thing we need to do is read the dataset and translate it to integers. What we'll do is to reuse the ```Field``` that records word indexes (the second output of ```get_data()```) and use it to parse the file.\n",
    "\n",
    "The wordsim data is structured as follows:\n",
    "\n",
    "```\n",
    "word1 word2 score\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "The ```Field``` we got from ```read_data()``` has two built-in functions, ```stoi``` which maps a string to an integer and ```itos``` which maps an integer to a string. \n",
    "\n",
    "What our datareader needs to do is: \n",
    "\n",
    "```\n",
    "for line in file:\n",
    "    word1, word2, score = file.split()\n",
    "    # encode word1 and word2 as integers\n",
    "    word1_idx = vocab.vocab.stoi[word1]\n",
    "    word2_idx = vocab.vocab.stoi[word2]\n",
    "```\n",
    "\n",
    "when we have the integers for ```word_1``` and ```word2``` we'll compute the similarity between their word embeddings with *cosine simlarity*. We can obtain the embeddings by querying the embedding layer of the model.\n",
    "\n",
    "We calculate the cosine similarity for each word pair in the dataset, then compute the pearson correlation between the similarities we obtained with the scores given in the dataset. \n",
    "\n",
    "[4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS:\n",
      "Pearson correlation:  0.128\n",
      "\n",
      "SUMMARY:\n",
      "Pair\tPsychology\tModel\tDf.\n",
      "tiger -- cat\t0.735\t0.164\t0.571\n",
      "tiger -- tiger\t1.0\t1.0\t0.0\n",
      "plane -- car\t0.577\t0.044\t0.533\n",
      "train -- car\t0.631\t0.17\t0.461\n",
      "television -- radio\t0.677\t0.102\t0.575\n",
      "media -- radio\t0.742\t0.035\t0.707\n",
      "bread -- butter\t0.619\t0.01\t0.609\n",
      "cucumber -- potato\t0.592\t0.089\t0.503\n",
      "doctor -- nurse\t0.7\t-0.072\t0.772\n",
      "professor -- doctor\t0.662\t0.241\t0.421\n",
      "student -- professor\t0.681\t0.127\t0.554\n",
      "smart -- stupid\t0.581\t0.099\t0.482\n",
      "wood -- forest\t0.773\t0.184\t0.589\n",
      "money -- cash\t0.915\t0.115\t0.8\n",
      "king -- queen\t0.858\t0.084\t0.774\n",
      "bishop -- rabbi\t0.669\t0.036\t0.633\n",
      "fuck -- sex\t0.944\t0.189\t0.755\n",
      "football -- soccer\t0.903\t0.091\t0.812\n",
      "football -- basketball\t0.681\t0.162\t0.519\n",
      "football -- tennis\t0.663\t0.204\t0.459\n",
      "physics -- chemistry\t0.735\t0.085\t0.65\n",
      "vodka -- gin\t0.846\t0.167\t0.679\n",
      "vodka -- brandy\t0.813\t0.188\t0.625\n",
      "drink -- eat\t0.687\t0.172\t0.515\n",
      "car -- automobile\t0.894\t-0.086\t0.98\n",
      "gem -- jewel\t0.896\t0.15\t0.746\n",
      "journey -- voyage\t0.929\t0.15\t0.779\n",
      "boy -- lad\t0.883\t0.017\t0.866\n",
      "coast -- shore\t0.91\t0.147\t0.763\n",
      "magician -- wizard\t0.902\t0.01\t0.892\n",
      "midday -- noon\t0.929\t0.082\t0.847\n",
      "food -- fruit\t0.752\t0.059\t0.693\n",
      "bird -- cock\t0.71\t0.055\t0.655\n",
      "bird -- crane\t0.738\t0.192\t0.546\n",
      "food -- rooster\t0.442\t0.009\t0.433\n",
      "money -- dollar\t0.842\t-0.018\t0.86\n",
      "money -- currency\t0.904\t0.068\t0.836\n",
      "tiger -- jaguar\t0.8\t0.131\t0.669\n",
      "tiger -- feline\t0.8\t0.08\t0.72\n",
      "tiger -- mammal\t0.685\t-0.106\t0.791\n",
      "tiger -- animal\t0.7\t0.173\t0.527\n",
      "tiger -- organism\t0.477\t0.082\t0.395\n",
      "tiger -- fauna\t0.562\t0.069\t0.493\n",
      "psychology -- psychiatry\t0.808\t-0.022\t0.83\n",
      "psychology -- science\t0.671\t0.077\t0.594\n",
      "psychology -- discipline\t0.558\t0.111\t0.447\n",
      "planet -- star\t0.845\t0.142\t0.703\n",
      "planet -- moon\t0.808\t0.218\t0.59\n",
      "planet -- sun\t0.802\t0.255\t0.547\n",
      "precedent -- example\t0.585\t0.064\t0.521\n",
      "precedent -- antecedent\t0.604\t0.053\t0.551\n",
      "cup -- tableware\t0.685\t0.053\t0.632\n",
      "cup -- artifact\t0.292\t0.208\t0.084\n",
      "cup -- object\t0.369\t0.054\t0.315\n",
      "cup -- entity\t0.215\t0.097\t0.118\n",
      "jaguar -- cat\t0.742\t0.139\t0.603\n",
      "jaguar -- car\t0.727\t0.111\t0.616\n",
      "mile -- kilometer\t0.866\t0.197\t0.669\n",
      "skin -- eye\t0.622\t0.092\t0.53\n",
      "century -- year\t0.759\t0.125\t0.634\n",
      "announcement -- news\t0.756\t0.18\t0.576\n",
      "doctor -- personnel\t0.5\t0.025\t0.475\n",
      "hospital -- infrastructure\t0.463\t0.155\t0.308\n",
      "life -- death\t0.788\t0.289\t0.499\n",
      "travel -- activity\t0.5\t0.066\t0.434\n",
      "type -- kind\t0.897\t0.051\t0.846\n",
      "street -- place\t0.644\t0.072\t0.572\n",
      "street -- avenue\t0.888\t0.188\t0.7\n",
      "street -- block\t0.688\t0.073\t0.615\n",
      "cell -- phone\t0.781\t0.047\t0.734\n",
      "dividend -- payment\t0.763\t0.07\t0.693\n",
      "calculation -- computation\t0.844\t0.151\t0.693\n",
      "profit -- loss\t0.763\t0.278\t0.485\n",
      "dollar -- buck\t0.922\t0.049\t0.873\n",
      "phone -- equipment\t0.713\t0.051\t0.662\n",
      "liquid -- water\t0.789\t0.267\t0.522\n",
      "marathon -- sprint\t0.747\t0.213\t0.534\n",
      "seafood -- food\t0.834\t0.142\t0.692\n",
      "seafood -- lobster\t0.87\t0.216\t0.654\n",
      "lobster -- food\t0.781\t0.111\t0.67\n",
      "lobster -- wine\t0.57\t0.242\t0.328\n",
      "championship -- tournament\t0.836\t0.23\t0.606\n",
      "man -- woman\t0.83\t0.143\t0.687\n",
      "man -- governor\t0.525\t0.118\t0.407\n",
      "murder -- manslaughter\t0.853\t0.16\t0.693\n",
      "opera -- performance\t0.688\t0.106\t0.582\n",
      "glass -- metal\t0.556\t0.145\t0.411\n",
      "aluminum -- metal\t0.783\t0.184\t0.599\n",
      "rock -- jazz\t0.759\t0.176\t0.583\n",
      "museum -- theater\t0.719\t0.189\t0.53\n",
      "monk -- oracle\t0.5\t-0.004\t0.504\n",
      "cup -- food\t0.5\t0.087\t0.413\n",
      "journal -- association\t0.497\t0.16\t0.337\n",
      "street -- children\t0.494\t-0.013\t0.507\n",
      "car -- flight\t0.494\t0.083\t0.411\n",
      "space -- chemistry\t0.488\t0.156\t0.332\n",
      "situation -- conclusion\t0.481\t0.39\t0.091\n",
      "word -- similarity\t0.475\t0.142\t0.333\n",
      "peace -- plan\t0.475\t0.088\t0.387\n",
      "consumer -- energy\t0.475\t0.24\t0.235\n",
      "ministry -- culture\t0.469\t0.018\t0.451\n",
      "smart -- student\t0.462\t0.006\t0.456\n",
      "investigation -- effort\t0.459\t0.269\t0.19\n",
      "image -- surface\t0.456\t0.234\t0.222\n",
      "life -- term\t0.45\t0.122\t0.328\n",
      "start -- match\t0.447\t0.194\t0.253\n",
      "computer -- news\t0.447\t0.042\t0.405\n",
      "board -- recommendation\t0.447\t0.07\t0.377\n",
      "lad -- brother\t0.446\t0.03\t0.416\n",
      "observation -- architecture\t0.438\t0.073\t0.365\n",
      "coast -- hill\t0.438\t0.237\t0.201\n",
      "deployment -- departure\t0.425\t0.023\t0.402\n",
      "benchmark -- index\t0.425\t0.141\t0.284\n",
      "attempt -- peace\t0.425\t0.161\t0.264\n",
      "consumer -- confidence\t0.413\t-0.004\t0.417\n",
      "start -- year\t0.406\t0.043\t0.363\n",
      "focus -- life\t0.406\t0.048\t0.358\n",
      "development -- issue\t0.397\t0.172\t0.225\n",
      "theater -- history\t0.391\t0.095\t0.296\n",
      "situation -- isolation\t0.388\t0.142\t0.246\n",
      "profit -- warning\t0.388\t0.149\t0.239\n",
      "media -- trading\t0.388\t0.103\t0.285\n",
      "chance -- credibility\t0.388\t0.126\t0.262\n",
      "precedent -- information\t0.385\t0.135\t0.25\n",
      "architecture -- century\t0.378\t0.091\t0.287\n",
      "population -- development\t0.375\t0.16\t0.215\n",
      "stock -- live\t0.373\t0.176\t0.197\n",
      "peace -- atmosphere\t0.369\t0.053\t0.316\n",
      "morality -- marriage\t0.369\t0.061\t0.308\n",
      "minority -- peace\t0.369\t0.311\t0.058\n",
      "atmosphere -- landscape\t0.369\t0.083\t0.286\n",
      "report -- gain\t0.363\t-0.001\t0.364\n",
      "music -- project\t0.363\t0.118\t0.245\n",
      "seven -- series\t0.356\t0.156\t0.2\n",
      "experience -- music\t0.347\t0.2\t0.147\n",
      "school -- center\t0.344\t0.176\t0.168\n",
      "five -- month\t0.338\t0.077\t0.261\n",
      "announcement -- production\t0.338\t0.25\t0.088\n",
      "morality -- importance\t0.331\t-0.017\t0.348\n",
      "money -- operation\t0.331\t0.023\t0.308\n",
      "delay -- news\t0.331\t0.131\t0.2\n",
      "governor -- interview\t0.325\t0.13\t0.195\n",
      "practice -- institution\t0.319\t0.225\t0.094\n",
      "century -- nation\t0.316\t0.089\t0.227\n",
      "coast -- forest\t0.315\t0.153\t0.162\n",
      "shore -- woodland\t0.308\t-0.033\t0.341\n",
      "drink -- car\t0.304\t0.043\t0.261\n",
      "president -- medal\t0.3\t0.124\t0.176\n",
      "prejudice -- recognition\t0.3\t0.212\t0.088\n",
      "viewer -- serial\t0.297\t0.219\t0.078\n",
      "peace -- insurance\t0.294\t0.068\t0.226\n",
      "media -- gain\t0.288\t-0.104\t0.392\n",
      "precedent -- cognition\t0.281\t0.287\t0.006\n",
      "announcement -- effort\t0.275\t0.142\t0.133\n",
      "line -- insurance\t0.269\t-0.056\t0.325\n",
      "crane -- implement\t0.269\t0.204\t0.065\n",
      "drink -- mother\t0.265\t0.028\t0.237\n",
      "opera -- industry\t0.263\t0.019\t0.244\n",
      "volunteer -- motto\t0.256\t0.233\t0.023\n",
      "listing -- proximity\t0.256\t0.22\t0.036\n",
      "precedent -- collection\t0.25\t0.126\t0.124\n",
      "cup -- article\t0.24\t0.107\t0.133\n",
      "problem -- airport\t0.238\t0.013\t0.225\n",
      "reason -- hypertension\t0.231\t0.183\t0.048\n",
      "direction -- combination\t0.225\t0.16\t0.065\n",
      "glass -- magician\t0.208\t0.143\t0.065\n",
      "cemetery -- woodland\t0.208\t0.03\t0.178\n",
      "possibility -- girl\t0.194\t0.179\t0.015\n",
      "cup -- substance\t0.192\t0.154\t0.038\n",
      "stock -- egg\t0.181\t0.206\t0.025\n",
      "month -- hotel\t0.181\t0.161\t0.02\n",
      "energy -- secretary\t0.181\t0.043\t0.138\n",
      "precedent -- group\t0.177\t-0.031\t0.208\n",
      "production -- hike\t0.175\t0.105\t0.07\n",
      "stock -- phone\t0.162\t0.113\t0.049\n",
      "holy -- sex\t0.162\t-0.039\t0.201\n",
      "drink -- ear\t0.131\t0.157\t0.026\n",
      "delay -- racism\t0.119\t0.2\t0.081\n",
      "stock -- life\t0.092\t0.17\t0.078\n",
      "stock -- jaguar\t0.092\t0.151\t0.059\n",
      "monk -- slave\t0.092\t0.153\t0.061\n",
      "lad -- wizard\t0.092\t-0.04\t0.132\n",
      "sugar -- approach\t0.088\t0.16\t0.072\n",
      "rooster -- voyage\t0.062\t0.184\t0.122\n",
      "noon -- string\t0.054\t0.064\t0.01\n",
      "chord -- smile\t0.054\t0.023\t0.031\n",
      "professor -- cucumber\t0.031\t-0.016\t0.047\n",
      "king -- cabbage\t0.023\t0.124\t0.101\n",
      "king -- cabbage\t0.023\t0.124\t0.101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "def read_wordsim(path, \n",
    "                 vocabulary=vocab.stoi, \n",
    "                 checker=vocab.itos,\n",
    "                 embeddings=cbow_model.embeddings.weight):\n",
    "    #https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "    \n",
    "    dataset_sims = []\n",
    "    model_sims = []\n",
    "    summary=\"Pair\\tPsychology\\tModel\\tDf.\\n\"\n",
    "    \n",
    "    with open(path) as f:\n",
    "        for line in f.read().split(\"\\n\"):\n",
    "            if len(line) != 0: #We do not want empty lines.\n",
    "                word1, word2, score = line.split() #splits by space by default\n",
    "\n",
    "            # get the index for the word\n",
    "            word1_idx = vocabulary[word1]\n",
    "            word2_idx = vocabulary[word2]            \n",
    "            \n",
    "            if (checker[word1_idx]=='<unk>', checker[word2_idx]=='<unk>') == (False, False):\n",
    "            #If two words are unknown to the model they will both be assigned \"<unk>\"\n",
    "            #and associated with the same embedding (u). The cosine similarity of u and u\n",
    "            #will be 1. I suggest that such scores should not be part of our evaluation.\n",
    "\n",
    "                score = float(score)\n",
    "                dataset_sims.append(score)\n",
    "\n",
    "                # get the embedding of the word\n",
    "                # the hidden layer will be a matrix of weights;\n",
    "                # use the index to identify the right row/column (?)\n",
    "                word1_emb = embeddings[word1_idx]\n",
    "                word2_emb = embeddings[word2_idx]\n",
    "\n",
    "                # compute cosine similarity, we'll use the version included in pytorch functional\n",
    "                # https://pytorch.org/docs/master/generated/torch.nn.functional.cosine_similarity.html\n",
    "                cosine_similarity = F.cosine_similarity(word1_emb, word2_emb, dim=0)\n",
    "                \n",
    "                #In order to identify \"best\" and \"worst performing word pairs\"; see below.\n",
    "                psy=round(score/10, 3)\n",
    "                mod=round(cosine_similarity.item(), 3)\n",
    "                dif=round(abs(psy-mod), 3)\n",
    "                summary+=f\"{word1} -- {word2}\\t{psy}\\t{mod}\\t{dif}\\n\"\n",
    "\n",
    "                model_sims.append(cosine_similarity.item())\n",
    "    \n",
    "    return dataset_sims, model_sims, summary\n",
    "\n",
    "path = 'eval_data/wordsim_similarity_goldstandard.txt'\n",
    "data, model, summary = read_wordsim(path)\n",
    "pearson_correlation = np.corrcoef(data, model)\n",
    "r = round(pearson_correlation[0][1], 3)\n",
    "            \n",
    "# the non-diagonals give the pearson correlation\n",
    "\n",
    "print(\"\\nRESULTS:\")\n",
    "print(\"Pearson correlation: \", r)\n",
    "print(\"\\nSUMMARY:\")\n",
    "print(summary)\n",
    "\n",
    "with open(f\"evaluations/{name_of_model}.txt\", mode=\"w\") as f:\n",
    "    f.write(f\"Pearson's correlation coefficient: {r}.\\n\")\n",
    "    f.write(f\"\\nSummary of performance for pairs:\\n\")\n",
    "    f.write(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think the model performs good or bad? Why?\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "My model is quite terrible! **Pearson correlation = 0.128.** There is almost no correlation between the psychologial assessments of word similarity and the model's estimate of similarity. This means that my model does do not assign high scores to \"truly\" similar pairs, and low scores to \"truly\" dissimilar ones. \n",
    "\n",
    "But it could be worse: there could have been 0 or a negative correlation (which was the case for some of occasions of training the model with other parameters).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the 10 best and 10 worst performing word pairs, can you see any patterns that explain why *these* are the best and worst word pairs?\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "In general, my model gives lower scores to pairs then these are given in psychological estimates. The table below show the Maximum, Minimum and Range for the model and the pyschological data (ignoring similariy estimates of the same word).\n",
    "\n",
    "Stats|Psych|Model|\n",
    "-----|-----|----- \n",
    "MIN  |0.023|-0.106|\n",
    "MAX  |0.944|0.390|\n",
    "RANGE|0.921|0.496|\n",
    "\n",
    "A consequence of this is that the model performs better for pairs given low scores in the psychologial data. It performs worse, when the psychologiical score is high. This pattern becomes clear from the following table which is the top ten and bottom ten pairs sorted by Difference (decending order). As we see this sorting on difference  closely maps psychological scores. To conclude, the main problem with the model is that it does not represent similar words as similar. There is probably a frequency explanation here as well: some words of similar pairs are unusal words likely to not have been trained enough trough our limited data set. \n",
    "\n",
    "\n",
    "Pair|PsyScore|ModScore|Difference\n",
    "----|--------|--------|----------\n",
    "car -- automobile|0.894|-0.086|0.98\n",
    "magician -- wizard|0.902|0.01|0.892\n",
    "dollar -- buck|0.922|0.049|0.873\n",
    "boy -- lad|0.883|0.017|0.866\n",
    "money -- dollar|0.842|-0.018|0.86\n",
    "midday -- noon|0.929|0.082|0.847\n",
    "type -- kind|0.897|0.051|0.846\n",
    "money -- currency|0.904|0.068|0.836\n",
    "psychology -- psychiatry|0.808|-0.022|0.83\n",
    "football -- soccer|0.903|0.091|0.812\n",
    "...|...|...|...\n",
    "cup -- substance|0.192|0.154|0.038\n",
    "listing -- proximity|0.256|0.22|0.036\n",
    "chord -- smile|0.054|0.023|0.031\n",
    "drink -- ear|0.131|0.157|0.026\n",
    "stock -- egg|0.181|0.206|0.025\n",
    "volunteer -- motto|0.256|0.233|0.023\n",
    "month -- hotel|0.181|0.161|0.02\n",
    "possibility -- girl|0.194|0.179|0.015\n",
    "noon -- string|0.054|0.064|0.01\n",
    "precedent -- cognition|0.281|0.287|0.006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways of improving the model we apply to WordSim353.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "The most obvious suggestion for improvement would be to test the model for varying (larger) data set, epochs and batch sizes. (Perhaps something should be done to avoid overfitting?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider a scenario where we use these embeddings in a downstream task, for example sentiment analysis (roughly: determining whether a sentence is positive or negative). \n",
    "\n",
    "Give some examples why the sentiment analysis model would benefit from our embeddnings and one examples why our embeddings could hur the performance of the sentiment model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "My model is a poor representation of language (meaning). There would be no value of such model in any downstream application. \n",
    "\n",
    "However, the general rationale for using word embeddings in sentiment analysis is that it improves performance. Since embeddings carry \"meaning\", but e.g. one-hot vectors do not, using these meaning representations (embeddings) can be useful in a task where we want to extract the attitudinal meaning expressed in text. Using embeddings, we will have a more informative data (input) for our sentiment classifcation task. \n",
    "\n",
    "A possible problem of using embeddings would perhaps be that they might carry a bias with respect to the sentiment classifcation task at hand. Consider an extrme example where we trained our embeedings on a standup comedy roast (https://en.wikipedia.org/wiki/Roast_(comedy)) full of ironi and slurs and then implement these embeddings in a sentiment analysis of *Guide Michelin*. For this task, perhaps a \"meaningless\" input would be better than the trained one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second part we'll build a simple LSTM language model. Your task is to construct a model which takes a sentence as input and predict the next word for each word in the sentence. For this you'll use the ```LSTM``` class provided by PyTorch (https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). You can read more about the LSTM here: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "NOTE!!!: Use the same dataset (wiki-cropus.txt) as before.\n",
    "\n",
    "Our setup is similar to before, we first encode the words as distributed representations then pass these to the LSTM and for each output we predict the next word.\n",
    "\n",
    "For this we'll build a new dataloader with torchtext, the file we pass to the dataloader should contain one sentence per line, with words separated by whitespace.\n",
    "\n",
    "```\n",
    "word_1, ..., word_n\n",
    "word_1, ..., word_k\n",
    "...\n",
    "```\n",
    "\n",
    "in this dataloader you want to make sure that each sentence begins with a ```<start>``` token and ends with a ```<end>``` token, there is a keyword argument in ```Field``` for this :). But other than that, as before you read the dataset and output a iterator over the dataset and a vocabulary. \n",
    "\n",
    "Implement the dataloader, language model and the training loop (the training loop will basically be the same as for word2vec).\n",
    "\n",
    "[12 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMmodel10k3b\n"
     ]
    }
   ],
   "source": [
    "#SETTING SOME PARAMETERS FOR TRAINING AND SAVING MODELS\n",
    "restrict_data = True\n",
    "n_lines = 10000\n",
    "n_lstm_batch = 3\n",
    "\n",
    "def shorty(txt_in, txt_out, max_lines=n_lines):\n",
    "    with open(txt_in, mode=\"r\") as f:\n",
    "        my_data = f.read().split(\"\\n\")\n",
    "    with open(txt_out, mode=\"w\") as f:\n",
    "        output=\"\\n\".join(my_data[:max_lines])\n",
    "        f.write(output)\n",
    "\n",
    "if restrict_data==True:\n",
    "    shorty('data/wiki-corpus.txt', 'data/wiki-corpus-short.txt')\n",
    "    data_path='data/wiki-corpus-short.txt'\n",
    "    name_LSTM_model = f\"LSTMmodel{str(n_lines)[:-3]}k{n_lstm_batch}b\"\n",
    "else:\n",
    "    data_path = 'data/wiki-corpus.txt'\n",
    "    name_LSTM_model = f\"LSTMmodel_full{n_lstm_batch}b\"\n",
    "\n",
    "print(name_LSTM_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs as before :)\n",
    "lm_hyperparameters = {'epochs':3,\n",
    "                      'batch_size':n_lstm_batch,\n",
    "                      'learning_rate':0.001,\n",
    "                      'embedding_dim':128,\n",
    "                      'output_dim':128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = 'data/wiki-corpus.txt'\n",
    "#data_path = 'data/wiki-corpus-short.txt' ##DEFINED ABOVE\n",
    "\n",
    "#from torchtext.legacy.data import Field, BucketIterator, Iterator, TabularDataset\n",
    "from torchtext.data import Field, BucketIterator, Iterator, TabularDataset\n",
    "\n",
    "def get_data2(\n",
    "    my_path = data_path,\n",
    "    batch_size = 3\n",
    "    ):\n",
    "    \n",
    "    whitespacer = lambda x: x.split(' ') #from: files/partofspeech-tagging_main.py on Canvas\n",
    "    \n",
    "    MY_FIELD = Field(\n",
    "        tokenize = whitespacer,\n",
    "        lower=True,\n",
    "        batch_first = True,\n",
    "        init_token=\"<start>\", \n",
    "        eos_token=\"<end>\"\n",
    "    )\n",
    "\n",
    "    train = TabularDataset(\n",
    "        path   = my_path,\n",
    "        #train  = file,\n",
    "        format = 'csv',\n",
    "        fields = [(\"sentence\", MY_FIELD)],\n",
    "        #skip_header = True,\n",
    "        #csv_reader_params = {'delimiter':'\\t', 'quotechar':'}'} \n",
    "    )\n",
    "\n",
    "    MY_FIELD.build_vocab(train)\n",
    "    \n",
    "    my_bucket = BucketIterator(\n",
    "        train,      \n",
    "        batch_size        = batch_size,\n",
    "        sort_within_batch = True,\n",
    "        sort_key          = lambda x: len(x.sentence),\n",
    "        shuffle           = True,\n",
    "        device            = device)    \n",
    "    \n",
    "    return my_bucket, MY_FIELD.vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_withLSTM(nn.Module):\n",
    "    def __init__(self, n_words, emb_dim, outp_dim):\n",
    "        super(LM_withLSTM, self).__init__()\n",
    "        self.embeddings = nn.Embedding(n_words, emb_dim)\n",
    "        self.LSTM = nn.LSTM(emb_dim, outp_dim, batch_first=True)\n",
    "        self.predict_word = nn.Linear(outp_dim, n_words)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        embedded_seq = self.embeddings(seq)\n",
    "        timestep_reprentation, *_ = self.LSTM(embedded_seq)\n",
    "        predicted_words = self.predict_word(timestep_reprentation)\n",
    "        \n",
    "        return predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.1123121539203445\n",
      "17.836240290415525\n",
      "28.877104440204526\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "# load data\n",
    "dataset, vocab = get_data2(batch_size = lm_hyperparameters[\"batch_size\"])\n",
    "#dataset, vocab = get_data()\n",
    "\n",
    "# build model and construct loss/optimizer\n",
    "lm_model = LM_withLSTM(len(vocab), \n",
    "                       lm_hyperparameters['embedding_dim'],\n",
    "                       lm_hyperparameters['output_dim'])\n",
    "lm_model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lm_model.parameters(), lr=lm_hyperparameters['learning_rate'])\n",
    "#here cbow model was provided . i changed to lm model ----\n",
    "\n",
    "# start training loop\n",
    "total_loss = 0\n",
    "for epoch in range(lm_hyperparameters['epochs']):\n",
    "    for i, batch in enumerate(dataset):\n",
    "        \n",
    "        # the strucure for each BATCH is:\n",
    "        # <start>, w0, ..., wn, <end>\n",
    "        sentence = batch.sentence\n",
    "     \n",
    "        # when training the model, at each input we predict the *NEXT* token\n",
    "        # consequently there is nothing to predict when we give the model \n",
    "        # <end> as input. \n",
    "        # thus, we do not want to give <end> as input to the model, select \n",
    "        # from each batch all tokens except the last. \n",
    "        # tip: use pytorch indexing/slicing (same as numpy) \n",
    "        # (https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html#operations-on-tensors)\n",
    "        # (https://jhui.github.io/2018/02/09/PyTorch-Basic-operations/)\n",
    "        input_sentence = sentence[:, :-1]\n",
    "        \n",
    "        # send your batch of sentences to the model\n",
    "        output = lm_model(input_sentence)\n",
    "        \n",
    "        # for each output, the model predict the NEXT token, so we have to reshape \n",
    "        # our dataset again. On timestep t, we evaluate on token t+1. That is,\n",
    "        # we never predict the <start> token ;) so this time, we select all but the first \n",
    "        # token from sentences (that is, all the tokens that we predict)\n",
    "        \n",
    "        gold_data = sentence[:, 1:]\n",
    "        \n",
    "        # the shape of the output and sentence variable need to be changed,\n",
    "        # for the loss function. Details are in the documentation.\n",
    "        # You can use .view(...,...) to reshape the tensors  \n",
    "\n",
    "        #loss = loss_fn(output, hot_gold)\n",
    "        \n",
    "        b=output.shape[0] # the size of the batch\n",
    "        w=output.shape[1] # the size of the sentence\n",
    "        v=output.shape[2] # the size of the vocabulary\n",
    "        \n",
    "        input_cel = output.reshape(b*w, v)\n",
    "        target_cel = gold_data.reshape(1, b*w).squeeze()\n",
    "        \n",
    "        # I did not manage this with method view (perhaps it is wrong)\n",
    "        \n",
    "        loss = loss_fn(input_cel, target_cel)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        print(total_loss/(i+1), end='\\r') \n",
    "        \n",
    "        # print average loss for the epoch\n",
    "        #print(total_loss/(i+1), end='\\r') \n",
    "      \n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # reset gradients\n",
    "        optimizer.zero_grad\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING THE MODEL\n",
    "#PATH = f\"models/{name_LSTM_model}.pt\"\n",
    "#torch.save(lm_model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR LOADING THE MODEL\n",
    "#my_lm_model = torch.load(PATH)\n",
    "#my_lm_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the language model\n",
    "\n",
    "We'll evaluate our model using the BLiMP dataset (https://github.com/alexwarstadt/blimp). The BLiMP dataset contains sets of linguistic minimal pairs for various syntactic and semantic phenomena, We'll evaluate our model on *existential quantifiers* (link: https://github.com/alexwarstadt/blimp/blob/master/data/existential_there_quantifiers_1.jsonl). This data, as the name suggests, investigate whether language models assign higher probability to *correct* usage of there-quantifiers. \n",
    "\n",
    "An example entry in the dataset is: \n",
    "\n",
    "```\n",
    "{\"sentence_good\": \"There was a documentary about music irritating Allison.\", \"sentence_bad\": \"There was each documentary about music irritating Allison.\", \"field\": \"semantics\", \"linguistics_term\": \"quantifiers\", \"UID\": \"existential_there_quantifiers_1\", \"simple_LM_method\": true, \"one_prefix_method\": false, \"two_prefix_method\": false, \"lexically_identical\": false, \"pairID\": \"0\"}\n",
    "```\n",
    "\n",
    "Download the dataset and build a datareader (similar to what you did for word2vec). The dataset structure you should aim for is (you don't need to worry about the other keys for this assignment):\n",
    "\n",
    "```\n",
    "good_sentence_1, bad_sentence_1\n",
    "...\n",
    "```\n",
    "\n",
    "your task now is to compare the probability assigned to the good sentence with to the probability assigned to the bad sentence. To compute a probability for a sentence we consider the product of the probabilities assigned to the *gold* tokens, remember, at timestep ```t``` we're predicting which token comes *next* e.g. ```t+1``` (basically, you do the same thing as you did when training).\n",
    "\n",
    "In rough pseudo code what your code should do is:\n",
    "\n",
    "```\n",
    "accuracy = []\n",
    "for good_sentence, bad_sentence in dataset:\n",
    "    gs_lm_output = LanguageModel(good_sentence)\n",
    "    gs_token_probabilities = softmax(gs_lm_output)\n",
    "    gs_sentence_probability = product(gs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    bs_lm_output = LanguageModel(bad_sentence)\n",
    "    bs_token_probabilities = softmax(bs_lm_output)\n",
    "    bs_sentence_probability = product(bs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    # int(True) = 1 and int(False) = 0\n",
    "    is_correct = int(gs_sentence_probability > bs_sentence_probability)\n",
    "    accuracy.append(is_correct)\n",
    "\n",
    "print(numpy.mean(accuracy))\n",
    "    \n",
    "```\n",
    "\n",
    "[6 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy:\n",
      "0.251\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "import json\n",
    "\n",
    "def evaluate_model(path, vocab, model):\n",
    "    \n",
    "    accuracy = []\n",
    "    with open(path) as f:\n",
    "        # iterate over one pair of sentences at a time\n",
    "        for line in f:\n",
    "            # load the data\n",
    "            data = json.loads(line)\n",
    "            good_s = data['sentence_good']\n",
    "            bad_s = data['sentence_bad']\n",
    "            \n",
    "            # the data is tokenized as whitespace\n",
    "            tok_good_s = [token.lower().replace(\".\", \"\") for token in good_s.split()]\n",
    "            tok_bad_s = [token.lower().replace(\".\", \"\") for token in bad_s.split()]\n",
    "            #THERE IS A FULL STOP (.) ON THE LAST TOKEN WHICH WE NEED TO REMOVE\n",
    "        \n",
    "            \n",
    "            # encode your words as integers using the vocab from the dataloader, size is (S)\n",
    "            # we use unsqueeze to create the batch dimension \n",
    "            # in this case our input is only ONE batch, so the size of the tensor becomes: \n",
    "            # (S) -> (1, S) as the model expects batches\n",
    "            enc_good_s = torch.tensor([vocab.stoi[x] for x in tok_good_s], device=device).unsqueeze(0)\n",
    "            enc_bad_s = torch.tensor([vocab.stoi[x] for x in tok_bad_s], device=device).unsqueeze(0)\n",
    "            \n",
    "            # pass your encoded sentences to the model and predict the next tokens\n",
    "            good_s_pred = model(enc_good_s)\n",
    "            bad_s_pred = model(enc_bad_s)\n",
    "            \n",
    "            # get probabilities with softmax\n",
    "            gs_probs = F.softmax(good_s_pred[0], dim=0)\n",
    "            bs_probs = F.softmax(bad_s_pred[0], dim=0)\n",
    "            \n",
    "            # select the probability of the gold tokens\n",
    "            gs_sent_prob = find_token_probs(gs_probs, enc_good_s)\n",
    "            bs_sent_prob = find_token_probs(bs_probs, enc_bad_s)\n",
    "            \n",
    "            accuracy.append(int(gs_sent_prob>bs_sent_prob))\n",
    "            \n",
    "    return accuracy\n",
    "            \n",
    "def find_token_probs(model_probs, encoded_sentence):\n",
    "    prob=1\n",
    "    for counter, idx in enumerate(encoded_sentence[0]):\n",
    "        token_prob=model_probs[counter][idx] #There is a softmax calculation for every word; pick the \"word-in-n-th-order\", then pick the probability for the nth word\n",
    "        prob *= token_prob\n",
    "    return prob     \n",
    "    \n",
    "path = 'eval_data/existential_there_quantifiers_1.jsonl'\n",
    "accuracy = evaluate_model(path, vocab, model=lm_model) #Note: provide your model\n",
    "\n",
    "print('Final accuracy:')\n",
    "print(np.round(np.mean(accuracy), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model get some score, say, 55% correct predictions. Is this good? Suggest some *baseline* (i.e. a stupid \"model\" we hope ours is better than) we can compare the model against.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "By chance alone, the probability of sentece S = {w1, w2, ..., wn}, given vocabulary size V, would be: \n",
    "\n",
    "*ReallyStupidModel1:*   P(S) = count(w1)/V * ... * count(wn)/V\n",
    "\n",
    "The model we build should at least perform better than this ReallyStupidModel. However, as the sentence pairs of the training data are \"minimal pairs\" differentiated by the quantifiers in them, the differntiation of sentence probabilities based on ReallyStupidModel whould come down to the frequency of the quantifiers in the corpus. \n",
    "\n",
    "A more sophisticated model for comparison should at least consider conditional proabilities of words given previous words, i.e. N-gram models:\n",
    "\n",
    "*NgramModel:*   P(S) = p(w1|w1-1, ... w1-N) * ... * p(wn|wn-1,... wn-N)\n",
    "\n",
    "Deciding on some value of *N*, which is problematic for Ngram models due to the \"curse of dimensionality\" (Bengio et al. 2003), our model should perform better than a *NgramModel*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some improvements you could make to your language model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "Again, as noted above, I have had problems with training the model with larger dataset and batch sizes. Supposedly, larger data sets and batch sizes would improve the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some other metrics we can use to evaluate our system\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "A metric for internal evaluation to consider is perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature\n",
    "\n",
    "\n",
    "Neural architectures:\n",
    "* Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. (Links to an external site.) Journal of Machine Learning Research, 3(6):1137–1155, 2003. (Sections 3 and 4 are less relevant today and hence you can glance through them quickly. Instead, look at the Mikolov papers where they describe training word embeddings with the current neural network architectures.)\n",
    "* T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n",
    "* T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total marks: 63"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
